{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1Wd6uMiTIgc-"],"authorship_tag":"ABX9TyPIxgwvXP53yz4ULAi/Y34a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Health Score Scraping and Extraction"],"metadata":{"id":"1Wd6uMiTIgc-"}},{"cell_type":"markdown","source":["First, we initialize all libraries and create our instances to extract and render from the SEC database, and make OpenAI requests."],"metadata":{"id":"QOuCJO38wStC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"VwkUy5wnvBKH","executionInfo":{"status":"ok","timestamp":1722786851171,"user_tz":240,"elapsed":16146,"user":{"displayName":"Liam Sheldon","userId":"11052288457284702954"}},"outputId":"06e8366b-1669-4638-b60a-8594b7733aec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.38.0-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n","Downloading openai-1.38.0-py3-none-any.whl (335 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.9/335.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.38.0\n","Collecting sec-api\n","  Downloading sec_api-1.0.18-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m752.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sec-api) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sec-api) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sec-api) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sec-api) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sec-api) (2024.7.4)\n","Downloading sec_api-1.0.18-py3-none-any.whl (17 kB)\n","Installing collected packages: sec-api\n","Successfully installed sec-api-1.0.18\n"]}],"source":["!pip install openai\n","!pip install sec-api\n","from google.colab import userdata\n","openai_key = userdata.get('openai')\n","sec_key = userdata.get('sec')\n","\n","import pandas as pd\n","import numpy as np\n","from openai import OpenAI\n","import json\n","from sec_api import ExtractorApi, RenderApi\n","\n","extractorApi = ExtractorApi(sec_key)\n","renderApi = RenderApi(sec_key)\n","client = OpenAI(api_key=openai_key)"]},{"cell_type":"markdown","source":["We first need to get a statement that details companies goals and current idealogies from their annual SEC report (10-K). We first got a list of the links to 5 companies 10-K's from the SEC's website."],"metadata":{"id":"xBzrJeaZFNiB"}},{"cell_type":"code","source":["tenk_links = json.load(open('10k_links_5_companies.json'))"],"metadata":{"id":"obs8hYvhFNDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we wrote a function to extract just section 1 from each 10-K, as this section usually contains information on companies goals, and added these to a new CSV."],"metadata":{"id":"i12TCVHhGbPE"}},{"cell_type":"code","source":["def get_goals(url):\n","    #Works on MCD < 2019, CMG, WEN, DMZ, TXRH\n","    business = extractorApi.get_section(url,\"1\",'text')\n","    return business"],"metadata":{"id":"TWIHXgWHGzax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we iterate through all the filings in the scraped tenk_links CSV, get the goals for the given 10-K, and add it to a new CSV."],"metadata":{"id":"5IZjQmXHG1Nj"}},{"cell_type":"code","source":["goals = []\n","for filing in tenk_links['filings']:\n","    # get_goals doesn't work on MCD > 2018, so we need to make sure it's not a MCD filing from those years\n","    if not(filing['filedAt'][:4] in {'2019', '2020', '2021'} and filing['ticker'] == 'MCD'):\n","        for form in filing['documentFormatFiles']:\n","            if form['type'] == '10-K':\n","                document_url = form['documentUrl'].replace(\"/ix?doc=\", \"\")\n","                goals.append((get_goals(document_url), filing['filedAt'], filing['ticker']))\n","                break\n","\n","goal_df = pd.DataFrame(goals, columns=['goals', 'date', 'ticker'])\n","goal_df.to_csv('company_goals.csv', index=False)"],"metadata":{"id":"ezxXhMoAGa-b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we needed to determine the companies devotion to healthier food based on their goals. We first tried an embedding route for natural language processing. This function gets creates an embedding vector for a given section of text using OpenAI's text-embedding-3-large model. We also define the cosine_similarity function to give us a similarity score between two vectors, allowing us to compare two embeddings."],"metadata":{"id":"-7d7lHmswpFd"}},{"cell_type":"code","source":["def get_embedding(text):\n","    newtext = text.replace(\"\\n\", \" \")\n","    if len(newtext) > 32000:\n","        newtext = newtext[:32000]\n","    response = client.embeddings.create(\n","        input=[newtext],\n","        model=\"text-embedding-3-large\"\n","    )\n","    embedding = response.data[0].embedding\n","    return np.array(embedding)\n","\n","def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"],"metadata":{"id":"Paze_3spwLpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To determine how much the text indicates a concern for healthier food, we wrote this function, which compares the embedding vector for the text to the embedding vector to two other statements."],"metadata":{"id":"S_RpqFyvCwNz"}},{"cell_type":"code","source":["from math import floor\n","\n","def healthy_goal_score(statement):\n","    healthier_food_related = get_embedding(\"One thing we care about is making our food healthier.\")\n","    not_healthier_food_related = get_embedding(\"We do not care about making our food healthier.\")\n","\n","    statement_embedding = get_embedding(statement)\n","\n","    pos = cosine_similarity(statement_embedding, healthier_food_related)\n","    neg = cosine_similarity(statement_embedding, not_healthier_food_related)\n","\n","    lindiff = (pos-neg+1)/2\n","\n","    return floor(lindiff*100)"],"metadata":{"id":"1rVVmQ7xCvT7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, since companies statements are usually quite long, we found the embeddings were often not similar to either reference, and the positive and negative similarities were very close to each other, which did not yield very interesting or accurate data. Thus, we decide to use OpenAI's GPT-4o-mini model to analyze text and give a score on dedication to healthier food."],"metadata":{"id":"dayeAweTDr92"}},{"cell_type":"code","source":["def chat_goal_score(statement):\n","    system_message = 'You are an expert in NLP. Given an excerpt of a certain companies current business position, please provide a score from 0 to 100 indicating how much they care about making their food healthier. Provide your answer as a JSON object of the form {\"score\": 50}.'\n","    classifier = client.chat.completions.create(\n","        model='gpt-4o-mini',\n","        messages=[\n","            {'role': 'system', 'content': system_message},\n","            {'role': 'user', 'content': statement}\n","            ],\n","        response_format={'type':'json_object'}\n","    )\n","    response = classifier.choices[0].message.content\n","    jsonresponse = json.loads(response)\n","    return jsonresponse['score']"],"metadata":{"id":"N9ZJiFFIEwTv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This method was much more successful, so we then loaded the saved goal data from the CSV, and applied our chat_goal_score function to every 10-K excerpt."],"metadata":{"id":"wifzWM0eHnGN"}},{"cell_type":"code","source":["goaldf = pd.read_csv('company_goals.csv')\n","goaldf['health_score'] = goaldf['goals'].apply(lambda x: chat_goal_score(x))\n","goaldf.to_csv('health_scores.csv', index=False)"],"metadata":{"id":"LOt89xeYFHex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statewide Food Law Data Scraping and Extraction"],"metadata":{"id":"YbLvQc_OIpzV"}},{"cell_type":"markdown","source":["First, we visited the NCSL website to obtain information on statewide laws. We got a list of all health related laws passed in states from 2014-2022, and copied it into a TXT file."],"metadata":{"id":"QN9ZJwq7I1sh"}},{"cell_type":"code","source":["with open('rawncslfull.txt') as f:\n","    fullncsl = f.readlines()"],"metadata":{"id":"uA_pKQwcIzVN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we iterated over this file and extracted the year and state from each entry. If \"Food Safety\" was in the topics, we added also added it to a seperate object that kept track of just the food safety laws."],"metadata":{"id":"Roa3fSo3JsqY"}},{"cell_type":"code","source":["from collections import defaultdict\n","food_laws_map = defaultdict(lambda: defaultdict(int))\n","all_laws_map = defaultdict(lambda: defaultdict(int))\n","for i, line in enumerate(fullncsl):\n","    if line[:4] in {'2014','2015','2016','2017','2018','2019','2020','2021','2022'}:\n","        year = line[:4]\n","        state = fullncsl[i-1][:2]\n","        topics = fullncsl[i+5][8:].replace('\\n','').split(', ')\n","        if 'Food Safety' in topics:\n","            food_laws_map[state][year] += 1\n","        all_laws_map[state][year] += 1"],"metadata":{"collapsed":true,"id":"-BQSy3ICKKSP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we calculate the portion of health laws that are food related by dividing the corresponding entries in the objects."],"metadata":{"id":"8-NT8fLaKs8M"}},{"cell_type":"code","source":["weighted_map = {state: {year: food_laws_map[state][year]/all_laws_map[state][year] for year in food_laws_map[state]} for state in food_laws_map}"],"metadata":{"id":"O4nO2BNSK_gZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we save this map to an XLSX file."],"metadata":{"id":"vVjrdFQQLAGZ"}},{"cell_type":"code","source":["out = pd.DataFrame(weighted_map).fillna(0).astype(float)\n","out = out.sort_index().sort_index(axis=1)\n","out.to_excel('weighted_food_safety_laws_by_state_and_year.xlsx', sheet_name='Food Safety Laws')"],"metadata":{"id":"eXyl20MqLFk6"},"execution_count":null,"outputs":[]}]}